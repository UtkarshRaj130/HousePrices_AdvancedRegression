{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in each column:\n",
      " MSZoning           4\n",
      "LotFrontage      486\n",
      "Alley           2721\n",
      "Utilities          2\n",
      "Exterior1st        1\n",
      "Exterior2nd        1\n",
      "MasVnrType      1766\n",
      "MasVnrArea        23\n",
      "BsmtQual          81\n",
      "BsmtCond          82\n",
      "BsmtExposure      82\n",
      "BsmtFinType1      79\n",
      "BsmtFinSF1         1\n",
      "BsmtFinType2      80\n",
      "BsmtFinSF2         1\n",
      "BsmtUnfSF          1\n",
      "TotalBsmtSF        1\n",
      "Electrical         1\n",
      "BsmtFullBath       2\n",
      "BsmtHalfBath       2\n",
      "KitchenQual        1\n",
      "Functional         2\n",
      "FireplaceQu     1420\n",
      "GarageType       157\n",
      "GarageYrBlt      159\n",
      "GarageFinish     159\n",
      "GarageCars         1\n",
      "GarageArea         1\n",
      "GarageQual       159\n",
      "GarageCond       159\n",
      "PoolQC          2909\n",
      "Fence           2348\n",
      "MiscFeature     2814\n",
      "SaleType           1\n",
      "SalePrice       1459\n",
      "dtype: int64\n",
      "\n",
      "Total Null Values:  17166\n",
      "\n",
      "Dropping features with more than 500 null values: []\n",
      "Data after one-hot encoding:\n",
      "    Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0   1          60         65.0     8450            7            5       2003   \n",
      "1   2          20         80.0     9600            6            8       1976   \n",
      "2   3          60         68.0    11250            7            5       2001   \n",
      "3   4          70         60.0     9550            7            5       1915   \n",
      "4   5          60         84.0    14260            8            5       2000   \n",
      "\n",
      "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLI  SaleType_ConLw  \\\n",
      "0          2003       196.0       706.0  ...           False           False   \n",
      "1          1976         0.0       978.0  ...           False           False   \n",
      "2          2002       162.0       486.0  ...           False           False   \n",
      "3          1970         0.0       216.0  ...           False           False   \n",
      "4          2000       350.0       655.0  ...           False           False   \n",
      "\n",
      "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_AdjLand  \\\n",
      "0         False         False         True                  False   \n",
      "1         False         False         True                  False   \n",
      "2         False         False         True                  False   \n",
      "3         False         False         True                  False   \n",
      "4         False         False         True                  False   \n",
      "\n",
      "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
      "0                 False                 False                  True   \n",
      "1                 False                 False                  True   \n",
      "2                 False                 False                  True   \n",
      "3                 False                 False                 False   \n",
      "4                 False                 False                  True   \n",
      "\n",
      "   SaleCondition_Partial  \n",
      "0                  False  \n",
      "1                  False  \n",
      "2                  False  \n",
      "3                  False  \n",
      "4                  False  \n",
      "\n",
      "[5 rows x 247 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3662\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3380\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 158\n",
      "[LightGBM] [Info] Start training from score 180717.091610\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3392\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 180407.575342\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3383\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 157\n",
      "[LightGBM] [Info] Start training from score 180007.375000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3406\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 182883.660103\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001631 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3390\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 157\n",
      "[LightGBM] [Info] Start training from score 180590.277397\n",
      "Training MSE: 27812231.279904917\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 181441.541952\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3175\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 181121.274090\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3166\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 179912.635974\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3164\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 182516.069593\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3168\n",
      "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 182235.731551\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3163\n",
      "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 181421.170053\n",
      "Validation MSE: 818977556.9339317\n",
      "The model may be overfitting.\n",
      "\n",
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Step 1: Import train.csv and test.csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Store the original test ID for reference later\n",
    "test_ids = test.index\n",
    "\n",
    "# Step 2: Concatenate the train and test datasets\n",
    "# Create a new column 'Source' to distinguish between train and test data\n",
    "train['Source'] = 1\n",
    "test['Source'] = 0\n",
    "\n",
    "# Concatenate train and test data\n",
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Step 3: Check for Null Values and analyze it in combined data\n",
    "null_values = combined.isnull().sum()\n",
    "print(\"Null Values in each column:\\n\", null_values[null_values > 0])\n",
    "print(\"\\nTotal Null Values: \", null_values.sum())\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = combined.select_dtypes(include=['number']).columns\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values for numeric columns with the median\n",
    "combined[numeric_cols] = combined[numeric_cols].fillna(combined[numeric_cols].median())\n",
    "\n",
    "# Fill missing values for categorical columns with the mode (most frequent value)\n",
    "for col in categorical_cols:\n",
    "    mode_val = combined[col].mode()[0]  # Get the most frequent value\n",
    "    combined[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Drop features with more than 500 null values\n",
    "null_values = combined.isnull().sum()\n",
    "features_to_drop = null_values[null_values > 500].index\n",
    "print(f\"\\nDropping features with more than 500 null values: {features_to_drop.tolist()}\")\n",
    "combined.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Update categorical columns after dropping features\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns \n",
    "\n",
    "# Step 4: One-hot encoding for each string-based feature\n",
    "# Apply one-hot encoding to categorical variables\n",
    "combined = pd.get_dummies(combined, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Data after one-hot encoding:\\n\", combined.head())\n",
    "\n",
    "# Step 5: Split combined data back into train and test sets\n",
    "train_data = combined[combined['Source'] == 1].drop('Source', axis=1)\n",
    "test_data = combined[combined['Source'] == 0].drop('Source', axis=1)\n",
    "\n",
    "# Ensure the columns match exactly between train and test data\n",
    "X_train = train_data.drop(columns=['SalePrice'])\n",
    "y_train = train_data['SalePrice']\n",
    "\n",
    "# Match columns in test set with the training set\n",
    "X_test = test_data[X_train.columns]\n",
    "\n",
    "# Step 6: Define base models and stacking model\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, random_state=42)),\n",
    "    ('lgbm', lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, random_seed=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Initialize the stacking model with Ridge as the final estimator\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "\n",
    "# Step 7: Train the stacking model on the entire training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_predictions = stacking_model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "print(f'Training MSE: {train_mse}')\n",
    "\n",
    "# Split the training data into training and validation sets to check for overfitting\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the stacking model on the full training set\n",
    "stacking_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_predictions = stacking_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "\n",
    "# Compare the training and validation MSE\n",
    "if train_mse < val_mse:\n",
    "    print('The model may be overfitting.')\n",
    "else:\n",
    "    print('The model seems to be generalizing well.')\n",
    "\n",
    "# Step 8: Predict on the test set\n",
    "y_test_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Handle any extremely large or small predictions\n",
    "y_test_pred = np.clip(y_test_pred, 0, np.percentile(y_test_pred, 99))\n",
    "\n",
    "# Step 9: Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': test_ids, 'SalePrice': y_test_pred})\n",
    "output.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 27812231.279904917\n"
     ]
    }
   ],
   "source": [
    "print(f'Training MSE: {train_mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
