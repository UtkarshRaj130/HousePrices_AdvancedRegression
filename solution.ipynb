{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Step 1: Import train.csv and test.csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Store the original test ID for reference later\n",
    "test_ids = test.index\n",
    "\n",
    "# Step 2: Concatenate the train and test datasets\n",
    "# Create a new column 'Source' to distinguish between train and test data\n",
    "train['Source'] = 1\n",
    "test['Source'] = 0\n",
    "\n",
    "# Concatenate train and test data\n",
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Step 3: Check for Null Values and analyze it in combined data\n",
    "null_values = combined.isnull().sum()\n",
    "print(\"Null Values in each column:\\n\", null_values[null_values > 0])\n",
    "print(\"\\nTotal Null Values: \", null_values.sum())\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = combined.select_dtypes(include=['number']).columns\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values for numeric columns with the median\n",
    "combined[numeric_cols] = combined[numeric_cols].fillna(combined[numeric_cols].median())\n",
    "\n",
    "# Fill missing values for categorical columns with the mode (most frequent value)\n",
    "for col in categorical_cols:\n",
    "    mode_val = combined[col].mode()[0]  # Get the most frequent value\n",
    "    combined[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Drop features with more than 500 null values\n",
    "null_values = combined.isnull().sum()\n",
    "features_to_drop = null_values[null_values > 500].index\n",
    "print(f\"\\nDropping features with more than 500 null values: {features_to_drop.tolist()}\")\n",
    "combined.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Update categorical columns after dropping features\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns \n",
    "\n",
    "# Step 4: One-hot encoding for each string-based feature\n",
    "# Apply one-hot encoding to categorical variables\n",
    "combined = pd.get_dummies(combined, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Data after one-hot encoding:\\n\", combined.head())\n",
    "\n",
    "# Step 5: Split combined data back into train and test sets\n",
    "train_data = combined[combined['Source'] == 1].drop('Source', axis=1)\n",
    "test_data = combined[combined['Source'] == 0].drop('Source', axis=1)\n",
    "\n",
    "# Ensure the columns match exactly between train and test data\n",
    "X_train = train_data.drop(columns=['SalePrice'])\n",
    "y_train = train_data['SalePrice']\n",
    "\n",
    "# Match columns in test set with the training set\n",
    "X_test = test_data[X_train.columns]\n",
    "\n",
    "# Step 6: Define base models and stacking model\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, random_state=42)),\n",
    "    ('lgbm', lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42)),\n",
    "    ('catboost', CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, random_seed=42, verbose=0))\n",
    "]\n",
    "\n",
    "# Initialize the stacking model with Ridge as the final estimator\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "\n",
    "# Train the stacking model on the entire training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict on the test set\n",
    "y_test_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Handle any extremely large or small predictions\n",
    "y_test_pred = np.clip(y_test_pred, 0, np.percentile(y_test_pred, 99))\n",
    "\n",
    "# Step 8: Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': test_ids, 'SalePrice': y_test_pred})\n",
    "output.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to test_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
