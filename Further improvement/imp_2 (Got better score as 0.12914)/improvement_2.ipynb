{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in each column:\n",
      " MSZoning           4\n",
      "LotFrontage      486\n",
      "Alley           2721\n",
      "Utilities          2\n",
      "Exterior1st        1\n",
      "Exterior2nd        1\n",
      "MasVnrType      1766\n",
      "MasVnrArea        23\n",
      "BsmtQual          81\n",
      "BsmtCond          82\n",
      "BsmtExposure      82\n",
      "BsmtFinType1      79\n",
      "BsmtFinSF1         1\n",
      "BsmtFinType2      80\n",
      "BsmtFinSF2         1\n",
      "BsmtUnfSF          1\n",
      "TotalBsmtSF        1\n",
      "Electrical         1\n",
      "BsmtFullBath       2\n",
      "BsmtHalfBath       2\n",
      "KitchenQual        1\n",
      "Functional         2\n",
      "FireplaceQu     1420\n",
      "GarageType       157\n",
      "GarageYrBlt      159\n",
      "GarageFinish     159\n",
      "GarageCars         1\n",
      "GarageArea         1\n",
      "GarageQual       159\n",
      "GarageCond       159\n",
      "PoolQC          2909\n",
      "Fence           2348\n",
      "MiscFeature     2814\n",
      "SaleType           1\n",
      "SalePrice       1459\n",
      "dtype: int64\n",
      "\n",
      "Total Null Values:  17166\n",
      "\n",
      "Dropping features with more than 500 null values: []\n",
      "Data after one-hot encoding:\n",
      "    Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
      "0   1          60         65.0     8450            7            5       2003   \n",
      "1   2          20         80.0     9600            6            8       1976   \n",
      "2   3          60         68.0    11250            7            5       2001   \n",
      "3   4          70         60.0     9550            7            5       1915   \n",
      "4   5          60         84.0    14260            8            5       2000   \n",
      "\n",
      "   YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_ConLI  SaleType_ConLw  \\\n",
      "0          2003       196.0       706.0  ...           False           False   \n",
      "1          1976         0.0       978.0  ...           False           False   \n",
      "2          2002       162.0       486.0  ...           False           False   \n",
      "3          1970         0.0       216.0  ...           False           False   \n",
      "4          2000       350.0       655.0  ...           False           False   \n",
      "\n",
      "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_AdjLand  \\\n",
      "0         False         False         True                  False   \n",
      "1         False         False         True                  False   \n",
      "2         False         False         True                  False   \n",
      "3         False         False         True                  False   \n",
      "4         False         False         True                  False   \n",
      "\n",
      "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
      "0                 False                 False                  True   \n",
      "1                 False                 False                  True   \n",
      "2                 False                 False                  True   \n",
      "3                 False                 False                 False   \n",
      "4                 False                 False                  True   \n",
      "\n",
      "   SaleCondition_Partial  \n",
      "0                  False  \n",
      "1                  False  \n",
      "2                  False  \n",
      "3                  False  \n",
      "4                  False  \n",
      "\n",
      "[5 rows x 247 columns]\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001483 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 181441.541952\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "4 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 2396, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 1776, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4833, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4882, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 2396, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\catboost\\core.py\", line 1776, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4833, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4882, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/train_lib/dir_helper.cpp:26: Can't create train tmp dir: tmp\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.83876536 0.83224763 0.8783835\n",
      " 0.86172532 0.86825403 0.864099   0.87755788]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3395\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 181441.541952\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3175\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 181121.274090\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3166\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 179912.635974\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3164\n",
      "[LightGBM] [Info] Number of data points in the train set: 934, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 182516.069593\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3168\n",
      "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 182235.731551\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3163\n",
      "[LightGBM] [Info] Number of data points in the train set: 935, number of used features: 145\n",
      "[LightGBM] [Info] Start training from score 181421.170053\n",
      "\n",
      "Validation Logarithmic RMSE: 0.13887872795569142\n",
      "Validation R² Score: 0.8966443443420854\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3662\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 162\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001411 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3380\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 158\n",
      "[LightGBM] [Info] Start training from score 180717.091610\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3392\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 180407.575342\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3383\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 157\n",
      "[LightGBM] [Info] Start training from score 180007.375000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3406\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 155\n",
      "[LightGBM] [Info] Start training from score 182883.660103\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3390\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 157\n",
      "[LightGBM] [Info] Start training from score 180590.277397\n",
      "\n",
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Step 1: Import train.csv and test.csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Store the original test ID for reference later\n",
    "test_ids = test.index\n",
    "\n",
    "# Step 2: Concatenate the train and test datasets\n",
    "# Create a new column 'Source' to distinguish between train and test data\n",
    "train['Source'] = 1\n",
    "test['Source'] = 0\n",
    "\n",
    "# Concatenate train and test data\n",
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Step 3: Check for Null Values and analyze it in combined data\n",
    "null_values = combined.isnull().sum()\n",
    "print(\"Null Values in each column:\\n\", null_values[null_values > 0])\n",
    "print(\"\\nTotal Null Values: \", null_values.sum())\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = combined.select_dtypes(include=['number']).columns\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill missing values for numeric columns with the median\n",
    "combined[numeric_cols] = combined[numeric_cols].fillna(combined[numeric_cols].median())\n",
    "\n",
    "# Fill missing values for categorical columns with the mode (most frequent value)\n",
    "for col in categorical_cols:\n",
    "    mode_val = combined[col].mode()[0]  # Get the most frequent value\n",
    "    combined[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Drop features with more than 500 null values\n",
    "null_values = combined.isnull().sum()\n",
    "features_to_drop = null_values[null_values > 500].index\n",
    "print(f\"\\nDropping features with more than 500 null values: {features_to_drop.tolist()}\")\n",
    "combined.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Update categorical columns after dropping features\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 4: One-hot encoding for each string-based feature\n",
    "# Apply one-hot encoding to categorical variables\n",
    "combined = pd.get_dummies(combined, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"Data after one-hot encoding:\\n\", combined.head())\n",
    "\n",
    "# Step 5: Split combined data back into train and test sets\n",
    "train_data = combined[combined['Source'] == 1].drop('Source', axis=1)\n",
    "test_data = combined[combined['Source'] == 0].drop('Source', axis=1)\n",
    "\n",
    "# Ensure the columns match exactly between train and test data\n",
    "X = train_data.drop(columns=['SalePrice'])\n",
    "y = train_data['SalePrice']\n",
    "\n",
    "# Step 6: Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Define base models with Random Search for hyperparameter tuning\n",
    "# Define the hyperparameter grid for each model\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': randint(500, 1500),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'max_depth': randint(3, 15)\n",
    "}\n",
    "\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': randint(500, 1500),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'num_leaves': randint(20, 40)\n",
    "}\n",
    "\n",
    "cat_param_grid = {\n",
    "    'iterations': randint(500, 1500),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'depth': randint(5, 15)\n",
    "}\n",
    "\n",
    "# Initialize the models\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "lgbm = lgb.LGBMRegressor(random_state=42)\n",
    "catboost = CatBoostRegressor(random_seed=42, verbose=0)\n",
    "\n",
    "# Initialize RandomizedSearchCV for each model\n",
    "xgb_random_search = RandomizedSearchCV(estimator=xgb, param_distributions=xgb_param_grid, \n",
    "                                       n_iter=10, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "lgbm_random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=lgb_param_grid, \n",
    "                                        n_iter=10, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "catboost_random_search = RandomizedSearchCV(estimator=catboost, param_distributions=cat_param_grid, \n",
    "                                            n_iter=10, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit Random Search for each model\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "lgbm_random_search.fit(X_train, y_train)\n",
    "catboost_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best models\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "best_lgbm = lgbm_random_search.best_estimator_\n",
    "best_catboost = catboost_random_search.best_estimator_\n",
    "\n",
    "# Step 8: Define the stacking model with the best hyperparameters\n",
    "estimators = [\n",
    "    ('xgb', best_xgb),\n",
    "    ('lgbm', best_lgbm),\n",
    "    ('catboost', best_catboost)\n",
    "]\n",
    "\n",
    "# Initialize the stacking model with Ridge as the final estimator\n",
    "stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "\n",
    "# Train the stacking model on the training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict on the validation set\n",
    "y_val_pred = stacking_model.predict(X_val)\n",
    "\n",
    "# Calculate the logarithm of the predictions and the actual values\n",
    "y_val_log_pred = np.log1p(y_val_pred)\n",
    "y_val_log_actual = np.log1p(y_val)\n",
    "\n",
    "# Step 10: Calculate Logarithmic RMSE and R² score\n",
    "log_rmse = np.sqrt(mean_squared_error(y_val_log_actual, y_val_log_pred))\n",
    "r2 = r2_score(y_val_log_actual, y_val_log_pred)\n",
    "\n",
    "print(f\"\\nValidation Logarithmic RMSE: {log_rmse}\")\n",
    "print(f\"Validation R² Score: {r2}\")\n",
    "\n",
    "# Step 11: Predict on the test set using the entire training data\n",
    "stacking_model.fit(X, y)  # Re-train on the entire dataset before predicting on the test set\n",
    "y_test_pred = stacking_model.predict(test_data[X_train.columns])\n",
    "\n",
    "# Handle any extremely large or small predictions\n",
    "y_test_pred = np.clip(y_test_pred, 0, np.percentile(y_test_pred, 99))\n",
    "\n",
    "# Step 12: Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': test_ids, 'SalePrice': y_test_pred})\n",
    "output.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"\\nPredictions saved to test_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
