{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping features with more than 500 null values: ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature', 'SalePrice']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined['TotalSF'] = combined['TotalBsmtSF'] + combined['GrLivArea']\n",
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_12336\\3817266081.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined['HouseAge'] = combined['YrSold'] - combined['YearBuilt']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SalePrice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SalePrice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Step 7: Encode categorical variables\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Use Target Encoding for categorical variables\u001b[39;00m\n\u001b[0;32m     64\u001b[0m encoder \u001b[38;5;241m=\u001b[39m TargetEncoder(cols\u001b[38;5;241m=\u001b[39mcategorical_cols)\n\u001b[1;32m---> 65\u001b[0m combined[categorical_cols] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(combined[categorical_cols], \u001b[43mcombined\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSalePrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Step 8: One-hot encoding (if applicable, else Target Encoding might suffice)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# If you still want to use one-hot for some categorical features with few categories\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# combined = pd.get_dummies(combined, columns=categorical_cols, drop_first=True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Step 9: Split combined data back into train and test sets\u001b[39;00m\n\u001b[0;32m     74\u001b[0m train_data \u001b[38;5;241m=\u001b[39m combined[combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\utkar\\.conda\\envs\\py310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SalePrice'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Step 1: Import train.csv and test.csv\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Step 2: Store the original test ID for reference later\n",
    "test_ids = test['Id']\n",
    "\n",
    "# Step 3: Concatenate the train and test datasets\n",
    "# Create a new column 'Source' to distinguish between train and test data\n",
    "train['Source'] = 1\n",
    "test['Source'] = 0\n",
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Step 4: Drop features with more than 500 null values\n",
    "null_values = combined.isnull().sum()\n",
    "features_to_drop = null_values[null_values > 500].index\n",
    "print(f\"\\nDropping features with more than 500 null values: {features_to_drop.tolist()}\")\n",
    "combined.drop(columns=features_to_drop, inplace=True)\n",
    "\n",
    "# Step 5: Handle missing values\n",
    "numeric_cols = combined.select_dtypes(include=['number']).columns\n",
    "categorical_cols = combined.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Optionally, create missing indicators\n",
    "for col in categorical_cols:\n",
    "    combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
    "for col in numeric_cols:\n",
    "    combined[col + '_missing'] = combined[col].isnull().astype(int)\n",
    "\n",
    "# Fill missing values\n",
    "combined[numeric_cols] = combined[numeric_cols].fillna(combined[numeric_cols].median())\n",
    "combined[categorical_cols] = combined[categorical_cols].fillna(combined[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Step 6: Feature Engineering (optional)\n",
    "# Example: Total Square Footage and House Age\n",
    "combined['TotalSF'] = combined['TotalBsmtSF'] + combined['GrLivArea']\n",
    "combined['HouseAge'] = combined['YrSold'] - combined['YearBuilt']\n",
    "\n",
    "# Log transform skewed features\n",
    "skewed_features = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'TotalSF']\n",
    "for feature in skewed_features:\n",
    "    if feature in combined.columns:\n",
    "        combined[feature] = np.log1p(combined[feature])\n",
    "\n",
    "# Step 7: Encode categorical variables\n",
    "# Use Target Encoding for categorical variables\n",
    "encoder = TargetEncoder(cols=categorical_cols)\n",
    "combined[categorical_cols] = encoder.fit_transform(combined[categorical_cols], combined['SalePrice'])\n",
    "\n",
    "# Step 8: One-hot encoding (if applicable, else Target Encoding might suffice)\n",
    "# If you still want to use one-hot for some categorical features with few categories\n",
    "# combined = pd.get_dummies(combined, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Alternatively, keep target-encoded values as numerical.\n",
    "\n",
    "# Step 9: Split combined data back into train and test sets\n",
    "train_data = combined[combined['Source'] == 1].drop(['Source', 'Id'], axis=1)\n",
    "test_data = combined[combined['Source'] == 0].drop(['Source', 'Id', 'SalePrice'], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop('SalePrice', axis=1)\n",
    "y = np.log1p(train_data['SalePrice'])\n",
    "\n",
    "# Step 10: Feature Selection (optional)\n",
    "# Optionally perform feature selection here\n",
    "\n",
    "# Step 11: Split the training data into train and validation sets using K-Fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    # Define base models\n",
    "    estimators = [\n",
    "        ('xgb', XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=10, random_state=42)),\n",
    "        ('lgbm', lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=31, random_state=42)),\n",
    "        ('catboost', CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=10, random_seed=42, verbose=0))\n",
    "    ]\n",
    "    \n",
    "    # Define stacking regressor\n",
    "    stacking_model = StackingRegressor(estimators=estimators, final_estimator=Ridge())\n",
    "    \n",
    "    # Fit the model\n",
    "    stacking_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict on validation\n",
    "    y_val_pred = stacking_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_val_pred))\n",
    "    cv_scores.append(rmse)\n",
    "    print(f'Fold RMSE: {rmse}')\n",
    "\n",
    "print(f'Average CV RMSE: {np.mean(cv_scores)}')\n",
    "\n",
    "# Step 12: Train on entire training data\n",
    "stacking_model.fit(X, y)\n",
    "\n",
    "# Step 13: Predict on test set\n",
    "y_test_pred = stacking_model.predict(test_data)\n",
    "\n",
    "# Inverse log transform\n",
    "y_test_pred = np.expm1(y_test_pred)\n",
    "\n",
    "# Clip predictions if necessary\n",
    "y_test_pred = np.clip(y_test_pred, 0, np.percentile(y_test_pred, 99))\n",
    "\n",
    "# Step 14: Save the predictions\n",
    "output = pd.DataFrame({'Id': test_ids, 'SalePrice': y_test_pred})\n",
    "output.to_csv('test_predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to test_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
